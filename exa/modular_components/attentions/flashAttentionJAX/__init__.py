from flash_attention import flash_attention
from cosine_sim_flash_attention import cosine_sim_flash_attention
from casual_flash_attention import causal_flash_attention
from rabe_attention import rabe_attention
from attention import attention, causal_attention, cosine_sim_attention

from utils import value_and_grad_difference, PRNGKeyGenerator

plain_attention = attention